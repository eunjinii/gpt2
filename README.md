# gpt2
Training Transformer models using pipeline parallelism

## 참고자료
1. **Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,** [https://arxiv.org/pdf/2104.04473.pdf](https://arxiv.org/pdf/2104.04473.pdf)
2. **TRAINING TRANSFORMER MODELS USING PIPELINE PARALLELISM,** [https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html](https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html)
3. **TRAINING TRANSFORMER MODELS USING DISTRIBUTED DATA PARALLEL AND PIPELINE PARALLELISM,** [https://pytorch.org/tutorials/advanced/ddp_pipeline.html](https://pytorch.org/tutorials/advanced/ddp_pipeline.html)
4. ****Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression,**** [https://arxiv.org/pdf/2301.09830.pdf](https://arxiv.org/pdf/2301.09830.pdf)